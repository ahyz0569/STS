{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "detectron2-train-ver3-customloader-200425",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1GVI_MqC2kf5uwm-JIM2MBs7KxvVkZps8",
      "authorship_tag": "ABX9TyN6M7zvfxwsPxepopI/lq6P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahyz0569/STS/blob/yunok/doc/notebook/detectron2_train_ver3_customloader_200425.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7dCQceweZuW",
        "colab_type": "text"
      },
      "source": [
        "# install detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGhsjSTDT0Z9",
        "colab_type": "code",
        "outputId": "473ecc59-e294-46ea-e441-4a9dc4ab9fe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# Or, to install it from a local clone:\n",
        "!git clone https://github.com/facebookresearch/detectron2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'detectron2'...\n",
            "remote: Enumerating objects: 4249, done.\u001b[K\n",
            "remote: Total 4249 (delta 0), reused 0 (delta 0), pack-reused 4249\u001b[K\n",
            "Receiving objects: 100% (4249/4249), 2.28 MiB | 5.95 MiB/s, done.\n",
            "Resolving deltas: 100% (2989/2989), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26tUwpgtXKTM",
        "colab_type": "code",
        "outputId": "e1002aef-94a7-4271-d4cb-d12954b6bc5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd detectron2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/detectron2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAGhTIcrXPbF",
        "colab_type": "code",
        "outputId": "43fdd1df-f395-4b33-ffbb-8f149f3be564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python -m pip install -e ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/detectron2\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (7.0.0)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/81/3b/40e876afde9f5ffa1cfdce10565aba85b0dc2e067ed551dfb566cfee6d4d/yacs-0.1.7-py3-none-any.whl\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (0.8.7)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (3.2.1)\n",
            "Collecting mock\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (4.38.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (2.2.1)\n",
            "Collecting fvcore\n",
            "  Downloading https://files.pythonhosted.org/packages/be/09/7565b3c7782d916907d881fda1ba52a074ef9c8cf921cc437fa2c076bb3b/fvcore-0.1.dev200424.tar.gz\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1.1) (1.3.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from yacs>=0.1.6->detectron2==0.1.1) (3.13)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1.1) (1.18.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1.1) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1.1) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1.1) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1.1) (2.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (46.1.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (1.6.0.post3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (1.28.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (0.9.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (2.21.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (1.12.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1.1) (0.34.2)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.1) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.1) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.1) (0.2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.1) (2020.4.5.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.1.1) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->detectron2==0.1.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.1.1) (3.1.0)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.dev200424-cp36-none-any.whl size=40523 sha256=5a2b738218032f4180f8db0e76fb4c094e8c6e6870550192589a235b39877fd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/20/6d/36730421a68849a49c4c7470411f9192b7fa5ec9093cf1c7b8\n",
            "Successfully built fvcore\n",
            "\u001b[31mERROR: fvcore 0.1.dev200424 has requirement pyyaml>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "Installing collected packages: yacs, mock, portalocker, fvcore, detectron2\n",
            "  Running setup.py develop for detectron2\n",
            "Successfully installed detectron2 fvcore-0.1.dev200424 mock-4.0.2 portalocker-1.7.0 yacs-0.1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXcu1Opxk4Hk",
        "colab_type": "code",
        "outputId": "931976e6-2278-4765-dcba-b4f5ee876114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "# name 'unicode' is not defined error 해결\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-7xo3um7s\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-7xo3um7s\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (46.1.3)\n",
            "Requirement already satisfied, skipping upgrade: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.16)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.18.3)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools==2.0) (1.12.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=275303 sha256=7bf6b4e3e82cc9f39eabd172950de2173e18600f50cda80ab6b3e1755e6f56dc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3dht3qlw/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Found existing installation: pycocotools 2.0.0\n",
            "    Uninstalling pycocotools-2.0.0:\n",
            "      Successfully uninstalled pycocotools-2.0.0\n",
            "Successfully installed pycocotools-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlY4QlBEdhBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install detectron2:\n",
        "# !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shn0u3mfeoUT",
        "colab_type": "text"
      },
      "source": [
        "# library import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRVRNVCGNWHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import copy\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bpp38gRAfO88",
        "colab_type": "text"
      },
      "source": [
        "# Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMDPyU1fqm99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"my_dataset\", {}, \"/content/drive/My Drive/Final-project/detectron2/train/output_train.json\", \"/content/drive/My Drive/Final-project/detectron2/train/train_images\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d_MaDmAfinK",
        "colab_type": "code",
        "outputId": "8e04e751-47a8-4542-c0c9-c99ee045a641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "ingre_metadata = MetadataCatalog.get(\"my_dataset\")\n",
        "ingre_metadata"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Metadata(evaluator_type='coco', image_root='/content/drive/My Drive/Final-project/detectron2/train/train_images', json_file='/content/drive/My Drive/Final-project/detectron2/train/output_train.json', name='my_dataset')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycuD-lxvfiqy",
        "colab_type": "code",
        "outputId": "15ea072a-f5a1-4fb7-c758-46ca6600fd8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "from detectron2.data import DatasetCatalog\n",
        "dataset_dicts = DatasetCatalog.get(\"my_dataset\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[04/26 10:50:29 d2.data.datasets.coco]: \u001b[0mLoading /content/drive/My Drive/Final-project/detectron2/train/output_train.json takes 1.49 seconds.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/26 10:50:29 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[04/26 10:50:29 d2.data.datasets.coco]: \u001b[0mLoaded 2944 images in COCO format from /content/drive/My Drive/Final-project/detectron2/train/output_train.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgyTVSDoNuWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_dicts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQpPnM9wYkqD",
        "colab_type": "text"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pI-3DhzYnuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "import os\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\n",
        "    # Same \n",
        "    '/content/detectron2/configs/COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml'\n",
        ")\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset\",)\n",
        "cfg.DATASETS.TEST = ()  # no metrics implemented for this dataset\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\")  # initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 8\n",
        "# cfg.SOLVER.BASE_LR = 0.02\n",
        "# cfg.SOLVER.MAX_ITER = (\n",
        "#     300\n",
        "# )  # 300 iterations seems good enough, but you can certainly train longer\n",
        "# cfg.SOLVER.STEPS = (70000, 90000)\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = (\n",
        "    128\n",
        ")  # faster, and good enough for this toy dataset\n",
        "# cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # 3 classes (data, fig, hazelnut)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 8\n",
        "\n",
        "cfg.SOLVER.CHECKPOINT_PERIOD = 2000\n",
        "cfg.OUTPUT_DIR = \"/content/drive/My Drive/Final-project/detectron2\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2U3xGnENu95",
        "colab_type": "text"
      },
      "source": [
        "# Prepare custom dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_E_mAaONuhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from detectron2.data import build_detection_train_loader\n",
        "# from detectron2.data import transforms as T\n",
        "# from detectron2.data import detection_utils as utils\n",
        "\n",
        "# def custom_mapper(dataset_dict):\n",
        "#     # Implement a mapper, similar to the default DatasetMapper, but with your own customizations\n",
        "#     dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
        "#     print(\"dataset_dict: \", dataset_dict)\n",
        "    \n",
        "#     image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
        "#     transform_list = [T.Resize((800, 800)),\n",
        "#                       T.RandomFlip(prob=0.6, horizontal=True, vertical=False),\n",
        "#                       T.RandomFlip(prob=0.6, horizontal=False, vertical=True),\n",
        "#                       # T.RandomRotation(angle=[10.0, 160.0], sample_style='range'),\n",
        "#                       # T.RandomContrast(0.7, 3.2),\n",
        "#                       T.RandomBrightness(0.6, 1.8)\n",
        "#                       ]\n",
        "#     image, transforms = T.apply_transform_gens(transform_list, image)\n",
        "#     dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
        "\n",
        "#     annos = [\n",
        "#         utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
        "#         for obj in dataset_dict.pop(\"annotations\")\n",
        "#         if obj.get(\"iscrowd\", 0) == 0\n",
        "#     ]\n",
        "#     instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
        "#     dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
        "#     return dataset_dict\n",
        "\n",
        "# data_loader = build_detection_train_loader(cfg, mapper=custom_mapper)\n",
        "# # use this dataloader instead of the default"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYjit1vnxBwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from fvcore.common.file_io import PathManager\n",
        "from PIL import Image\n",
        "\n",
        "from detectron2.data import transforms as T\n",
        "from detectron2.data import detection_utils as utils\n",
        "\n",
        "\"\"\"\n",
        "This file contains the default mapping that's applied to \"dataset dicts\".\n",
        "\"\"\"\n",
        "\n",
        "__all__ = [\"DatasetMapper\"]\n",
        "\n",
        "\n",
        "class Custom_DatasetMapper:\n",
        "    \"\"\"\n",
        "    A callable which takes a dataset dict in Detectron2 Dataset format,\n",
        "    and map it into a format used by the model.\n",
        "    This is the default callable to be used to map your dataset dict into training data.\n",
        "    You may need to follow it to implement your own one for customized logic,\n",
        "    such as a different way to read or transform images.\n",
        "    See :doc:`/tutorials/data_loading` for details.\n",
        "    The callable currently does the following:\n",
        "    1. Read the image from \"file_name\"\n",
        "    2. Applies cropping/geometric transforms to the image and annotations\n",
        "    3. Prepare data and annotations to Tensor and :class:`Instances`\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg, is_train=True):\n",
        "        if cfg.INPUT.CROP.ENABLED and is_train:\n",
        "            self.crop_gen = T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE)\n",
        "            logging.getLogger(__name__).info(\"CropGen used in training: \" + str(self.crop_gen))\n",
        "        else:\n",
        "            self.crop_gen = None\n",
        "\n",
        "        self.tfm_gens = utils.build_transform_gen(cfg, is_train)\n",
        "\n",
        "        # fmt: off\n",
        "        self.img_format     = cfg.INPUT.FORMAT\n",
        "        self.mask_on        = cfg.MODEL.MASK_ON\n",
        "        self.mask_format    = cfg.INPUT.MASK_FORMAT\n",
        "        self.keypoint_on    = cfg.MODEL.KEYPOINT_ON\n",
        "        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS\n",
        "        # fmt: on\n",
        "        if self.keypoint_on and is_train:\n",
        "            # Flip only makes sense in training\n",
        "            self.keypoint_hflip_indices = utils.create_keypoint_hflip_indices(cfg.DATASETS.TRAIN)\n",
        "        else:\n",
        "            self.keypoint_hflip_indices = None\n",
        "\n",
        "        if self.load_proposals:\n",
        "            self.min_box_side_len = cfg.MODEL.PROPOSAL_GENERATOR.MIN_SIZE\n",
        "            self.proposal_topk = (\n",
        "                cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TRAIN\n",
        "                if is_train\n",
        "                else cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TEST\n",
        "            )\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def __call__(self, dataset_dict):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.\n",
        "        Returns:\n",
        "            dict: a format that builtin models in detectron2 accept\n",
        "        \"\"\"\n",
        "        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
        "        image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
        "        transform_list = [T.Resize((800, 800)),\n",
        "                          T.ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'),\n",
        "                        T.RandomFlip(prob=0.6, horizontal=True, vertical=False),\n",
        "                        T.RandomFlip(prob=0.6, horizontal=False, vertical=True),\n",
        "                        # T.RandomCrop(crop_type=\"relative_range\", crop_size=(400, 400)),\n",
        "                        T.RandomRotation(angle=[10.0, 160.0]),\n",
        "                        # T.RandomContrast(0.7, 3.2),\n",
        "                        T.RandomBrightness(0.9, 1.1)\n",
        "                        ]\n",
        "        image, transforms = T.apply_transform_gens(transform_list, image)\n",
        "\n",
        "        # image aug check\n",
        "        # flag = cv2.imwrite(\"/content/drive/My Drive/aa.jpg\", image)\n",
        "        # print(flag)\n",
        "        \n",
        "        dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
        "        \n",
        "        annos = [\n",
        "            utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
        "            for obj in dataset_dict.pop(\"annotations\")\n",
        "            if obj.get(\"iscrowd\", 0) == 0\n",
        "        ]\n",
        "        instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
        "        dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
        "        return dataset_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52eFIOYIfy5g",
        "colab_type": "text"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S0F-cJVaBrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.data import build_detection_train_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atGGmbaPW-5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "class Custom_Trainer(DefaultTrainer):\n",
        "\n",
        "    @classmethod\n",
        "    def build_test_loader(cls, cfg, dataset_name):\n",
        "        return build_detection_test_loader(cfg, dataset_name, mapper=DatasetMapper(cfg, False))\n",
        "\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        return build_detection_train_loader(cfg, mapper=Custom_DatasetMapper(cfg, True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noCKULKGvD44",
        "colab_type": "code",
        "outputId": "65a53e3f-d01c-4856-cf5c-c5151b3a05a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "# trainer = DefaultTrainer(cfg)\n",
        "trainer = Custom_Trainer(cfg)\n",
        "trainer.resume_or_load(resume=True)\n",
        "trainer.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[04/26 10:50:39 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=9, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=32, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[04/26 10:50:39 d2.data.detection_utils]: \u001b[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/26 10:50:39 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[04/26 10:50:39 d2.data.datasets.coco]: \u001b[0mLoaded 2944 images in COCO format from /content/drive/My Drive/Final-project/detectron2/train/output_train.json\n",
            "\u001b[32m[04/26 10:50:39 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 2944 images left.\n",
            "\u001b[32m[04/26 10:50:39 d2.data.build]: \u001b[0mDistribution of instances among all 8 categories:\n",
            "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|   chilli   | 740          |    egg     | 843          |  porkmeat  | 618          |\n",
            "|   potato   | 1131         |     pa     | 547          |   onion    | 140          |\n",
            "|   carrot   | 1191         |  cucumber  | 489          |            |              |\n",
            "|   total    | 5699         |            |              |            |              |\u001b[0m\n",
            "\u001b[32m[04/26 10:50:39 d2.data.common]: \u001b[0mSerializing 2944 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[04/26 10:50:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.86 MiB\n",
            "\u001b[32m[04/26 10:50:39 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[04/26 10:50:51 d2.engine.train_loop]: \u001b[0mStarting training from iteration 26000\n",
            "\u001b[32m[04/26 10:52:08 d2.utils.events]: \u001b[0m eta: 1 day, 16:02:23  iter: 26019  total_loss: 0.368  loss_cls: 0.106  loss_box_reg: 0.256  loss_rpn_cls: 0.002  loss_rpn_loc: 0.010  time: 2.5304  data_time: 3.0515  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 10:52:55 d2.utils.events]: \u001b[0m eta: 1 day, 17:06:14  iter: 26039  total_loss: 0.367  loss_cls: 0.103  loss_box_reg: 0.237  loss_rpn_cls: 0.002  loss_rpn_loc: 0.010  time: 2.4050  data_time: 1.5530  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 10:53:44 d2.utils.events]: \u001b[0m eta: 1 day, 17:46:00  iter: 26059  total_loss: 0.372  loss_cls: 0.119  loss_box_reg: 0.253  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 2.4259  data_time: 1.6470  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 10:54:30 d2.utils.events]: \u001b[0m eta: 1 day, 17:27:02  iter: 26079  total_loss: 0.395  loss_cls: 0.106  loss_box_reg: 0.270  loss_rpn_cls: 0.002  loss_rpn_loc: 0.009  time: 2.3955  data_time: 1.4495  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 10:55:20 d2.utils.events]: \u001b[0m eta: 1 day, 17:07:14  iter: 26099  total_loss: 0.363  loss_cls: 0.112  loss_box_reg: 0.238  loss_rpn_cls: 0.002  loss_rpn_loc: 0.008  time: 2.4105  data_time: 1.6799  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 10:56:06 d2.utils.events]: \u001b[0m eta: 1 day, 16:45:59  iter: 26119  total_loss: 0.372  loss_cls: 0.110  loss_box_reg: 0.230  loss_rpn_cls: 0.002  loss_rpn_loc: 0.008  time: 2.3927  data_time: 1.4283  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 10:57:01 d2.utils.events]: \u001b[0m eta: 1 day, 17:05:41  iter: 26139  total_loss: 0.404  loss_cls: 0.132  loss_box_reg: 0.264  loss_rpn_cls: 0.004  loss_rpn_loc: 0.008  time: 2.4455  data_time: 1.8757  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 10:57:48 d2.utils.events]: \u001b[0m eta: 1 day, 17:04:55  iter: 26159  total_loss: 0.442  loss_cls: 0.143  loss_box_reg: 0.276  loss_rpn_cls: 0.004  loss_rpn_loc: 0.012  time: 2.4361  data_time: 1.5111  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 10:58:34 d2.utils.events]: \u001b[0m eta: 1 day, 16:22:40  iter: 26179  total_loss: 0.422  loss_cls: 0.110  loss_box_reg: 0.275  loss_rpn_cls: 0.004  loss_rpn_loc: 0.010  time: 2.4188  data_time: 1.3923  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 10:59:24 d2.utils.events]: \u001b[0m eta: 1 day, 16:21:54  iter: 26199  total_loss: 0.345  loss_cls: 0.111  loss_box_reg: 0.226  loss_rpn_cls: 0.004  loss_rpn_loc: 0.011  time: 2.4269  data_time: 1.6521  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:00:20 d2.utils.events]: \u001b[0m eta: 1 day, 16:42:09  iter: 26219  total_loss: 0.387  loss_cls: 0.109  loss_box_reg: 0.254  loss_rpn_cls: 0.002  loss_rpn_loc: 0.009  time: 2.4596  data_time: 1.8734  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:01:06 d2.utils.events]: \u001b[0m eta: 1 day, 16:34:32  iter: 26239  total_loss: 0.330  loss_cls: 0.090  loss_box_reg: 0.216  loss_rpn_cls: 0.003  loss_rpn_loc: 0.012  time: 2.4468  data_time: 1.4669  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:01:54 d2.utils.events]: \u001b[0m eta: 1 day, 17:01:03  iter: 26259  total_loss: 0.336  loss_cls: 0.103  loss_box_reg: 0.232  loss_rpn_cls: 0.002  loss_rpn_loc: 0.009  time: 2.4432  data_time: 1.5585  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:02:42 d2.utils.events]: \u001b[0m eta: 1 day, 16:33:01  iter: 26279  total_loss: 0.348  loss_cls: 0.094  loss_box_reg: 0.233  loss_rpn_cls: 0.003  loss_rpn_loc: 0.011  time: 2.4427  data_time: 1.6913  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:03:34 d2.utils.events]: \u001b[0m eta: 1 day, 16:32:15  iter: 26299  total_loss: 0.401  loss_cls: 0.123  loss_box_reg: 0.256  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 2.4510  data_time: 1.7427  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:04:24 d2.utils.events]: \u001b[0m eta: 1 day, 16:31:29  iter: 26319  total_loss: 0.384  loss_cls: 0.128  loss_box_reg: 0.250  loss_rpn_cls: 0.002  loss_rpn_loc: 0.008  time: 2.4534  data_time: 1.6872  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:05:13 d2.utils.events]: \u001b[0m eta: 1 day, 17:09:41  iter: 26339  total_loss: 0.383  loss_cls: 0.103  loss_box_reg: 0.244  loss_rpn_cls: 0.004  loss_rpn_loc: 0.010  time: 2.4538  data_time: 1.6254  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:06:01 d2.utils.events]: \u001b[0m eta: 1 day, 17:08:54  iter: 26359  total_loss: 0.414  loss_cls: 0.116  loss_box_reg: 0.246  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 2.4517  data_time: 1.5794  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:06:35 d2.utils.events]: \u001b[0m eta: 1 day, 16:13:14  iter: 26379  total_loss: 0.364  loss_cls: 0.104  loss_box_reg: 0.243  loss_rpn_cls: 0.004  loss_rpn_loc: 0.007  time: 2.4109  data_time: 0.7030  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:07:01 d2.utils.events]: \u001b[0m eta: 1 day, 14:43:08  iter: 26399  total_loss: 0.386  loss_cls: 0.130  loss_box_reg: 0.264  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 2.3548  data_time: 0.2673  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:07:28 d2.utils.events]: \u001b[0m eta: 1 day, 13:35:37  iter: 26419  total_loss: 0.399  loss_cls: 0.136  loss_box_reg: 0.225  loss_rpn_cls: 0.004  loss_rpn_loc: 0.009  time: 2.3072  data_time: 0.2812  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:07:55 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:27  iter: 26439  total_loss: 0.507  loss_cls: 0.141  loss_box_reg: 0.304  loss_rpn_cls: 0.005  loss_rpn_loc: 0.013  time: 2.2642  data_time: 0.2727  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:08:23 d2.utils.events]: \u001b[0m eta: 1 day, 10:46:34  iter: 26459  total_loss: 0.406  loss_cls: 0.118  loss_box_reg: 0.270  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 2.2259  data_time: 0.2843  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:08:50 d2.utils.events]: \u001b[0m eta: 1 day, 10:18:25  iter: 26479  total_loss: 0.400  loss_cls: 0.117  loss_box_reg: 0.264  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 2.1903  data_time: 0.2963  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:09:18 d2.utils.events]: \u001b[0m eta: 1 day, 7:51:14  iter: 26499  total_loss: 0.363  loss_cls: 0.111  loss_box_reg: 0.241  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 2.1583  data_time: 0.3040  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:09:46 d2.utils.events]: \u001b[0m eta: 1 day, 6:07:34  iter: 26519  total_loss: 0.397  loss_cls: 0.114  loss_box_reg: 0.258  loss_rpn_cls: 0.002  loss_rpn_loc: 0.010  time: 2.1280  data_time: 0.2809  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:10:13 d2.utils.events]: \u001b[0m eta: 1 day, 3:38:23  iter: 26539  total_loss: 0.364  loss_cls: 0.100  loss_box_reg: 0.248  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 2.0991  data_time: 0.2858  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:10:39 d2.utils.events]: \u001b[0m eta: 1 day, 1:56:04  iter: 26559  total_loss: 0.343  loss_cls: 0.099  loss_box_reg: 0.233  loss_rpn_cls: 0.003  loss_rpn_loc: 0.007  time: 2.0710  data_time: 0.2798  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:11:06 d2.utils.events]: \u001b[0m eta: 1 day, 1:46:44  iter: 26579  total_loss: 0.379  loss_cls: 0.106  loss_box_reg: 0.235  loss_rpn_cls: 0.003  loss_rpn_loc: 0.012  time: 2.0458  data_time: 0.3003  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:11:34 d2.utils.events]: \u001b[0m eta: 1 day, 1:38:03  iter: 26599  total_loss: 0.352  loss_cls: 0.106  loss_box_reg: 0.221  loss_rpn_cls: 0.001  loss_rpn_loc: 0.008  time: 2.0236  data_time: 0.2791  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:12:01 d2.utils.events]: \u001b[0m eta: 1 day, 1:17:27  iter: 26619  total_loss: 0.365  loss_cls: 0.105  loss_box_reg: 0.247  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 2.0023  data_time: 0.2688  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:12:28 d2.utils.events]: \u001b[0m eta: 1 day, 1:14:06  iter: 26639  total_loss: 0.414  loss_cls: 0.140  loss_box_reg: 0.262  loss_rpn_cls: 0.005  loss_rpn_loc: 0.010  time: 1.9822  data_time: 0.2726  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:12:55 d2.utils.events]: \u001b[0m eta: 1 day, 1:08:17  iter: 26659  total_loss: 0.389  loss_cls: 0.112  loss_box_reg: 0.260  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.9631  data_time: 0.2612  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:13:23 d2.utils.events]: \u001b[0m eta: 1 day, 1:01:40  iter: 26679  total_loss: 0.358  loss_cls: 0.110  loss_box_reg: 0.232  loss_rpn_cls: 0.002  loss_rpn_loc: 0.008  time: 1.9454  data_time: 0.2598  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:13:50 d2.utils.events]: \u001b[0m eta: 1 day, 0:54:36  iter: 26699  total_loss: 0.420  loss_cls: 0.133  loss_box_reg: 0.273  loss_rpn_cls: 0.004  loss_rpn_loc: 0.011  time: 1.9287  data_time: 0.2921  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:14:17 d2.utils.events]: \u001b[0m eta: 1 day, 0:50:12  iter: 26719  total_loss: 0.373  loss_cls: 0.105  loss_box_reg: 0.241  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.9134  data_time: 0.2834  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:14:44 d2.utils.events]: \u001b[0m eta: 1 day, 0:45:29  iter: 26739  total_loss: 0.407  loss_cls: 0.121  loss_box_reg: 0.275  loss_rpn_cls: 0.004  loss_rpn_loc: 0.009  time: 1.8981  data_time: 0.2704  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:15:11 d2.utils.events]: \u001b[0m eta: 1 day, 0:42:59  iter: 26759  total_loss: 0.380  loss_cls: 0.110  loss_box_reg: 0.261  loss_rpn_cls: 0.003  loss_rpn_loc: 0.007  time: 1.8829  data_time: 0.2869  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:15:39 d2.utils.events]: \u001b[0m eta: 1 day, 0:42:38  iter: 26779  total_loss: 0.364  loss_cls: 0.124  loss_box_reg: 0.233  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.8708  data_time: 0.2993  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:16:07 d2.utils.events]: \u001b[0m eta: 1 day, 0:41:48  iter: 26799  total_loss: 0.375  loss_cls: 0.120  loss_box_reg: 0.255  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.8584  data_time: 0.2830  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:16:34 d2.utils.events]: \u001b[0m eta: 1 day, 0:40:35  iter: 26819  total_loss: 0.360  loss_cls: 0.101  loss_box_reg: 0.235  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.8466  data_time: 0.2676  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:17:02 d2.utils.events]: \u001b[0m eta: 1 day, 0:39:48  iter: 26839  total_loss: 0.363  loss_cls: 0.103  loss_box_reg: 0.230  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.8362  data_time: 0.3291  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:17:30 d2.utils.events]: \u001b[0m eta: 1 day, 0:39:39  iter: 26859  total_loss: 0.366  loss_cls: 0.104  loss_box_reg: 0.249  loss_rpn_cls: 0.002  loss_rpn_loc: 0.010  time: 1.8261  data_time: 0.2957  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:17:58 d2.utils.events]: \u001b[0m eta: 1 day, 0:38:49  iter: 26879  total_loss: 0.393  loss_cls: 0.110  loss_box_reg: 0.267  loss_rpn_cls: 0.002  loss_rpn_loc: 0.011  time: 1.8157  data_time: 0.2789  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:18:26 d2.utils.events]: \u001b[0m eta: 1 day, 0:38:17  iter: 26899  total_loss: 0.380  loss_cls: 0.108  loss_box_reg: 0.228  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.8061  data_time: 0.2891  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:18:53 d2.utils.events]: \u001b[0m eta: 1 day, 0:36:47  iter: 26919  total_loss: 0.315  loss_cls: 0.095  loss_box_reg: 0.207  loss_rpn_cls: 0.003  loss_rpn_loc: 0.007  time: 1.7965  data_time: 0.2928  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:19:20 d2.utils.events]: \u001b[0m eta: 1 day, 0:34:47  iter: 26939  total_loss: 0.447  loss_cls: 0.124  loss_box_reg: 0.270  loss_rpn_cls: 0.004  loss_rpn_loc: 0.011  time: 1.7866  data_time: 0.2736  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:19:47 d2.utils.events]: \u001b[0m eta: 1 day, 0:33:11  iter: 26959  total_loss: 0.393  loss_cls: 0.125  loss_box_reg: 0.273  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.7777  data_time: 0.2767  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:20:15 d2.utils.events]: \u001b[0m eta: 1 day, 0:32:03  iter: 26979  total_loss: 0.381  loss_cls: 0.108  loss_box_reg: 0.239  loss_rpn_cls: 0.003  loss_rpn_loc: 0.011  time: 1.7697  data_time: 0.3271  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:20:41 d2.utils.events]: \u001b[0m eta: 1 day, 0:30:16  iter: 26999  total_loss: 0.355  loss_cls: 0.119  loss_box_reg: 0.225  loss_rpn_cls: 0.003  loss_rpn_loc: 0.012  time: 1.7608  data_time: 0.2781  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:21:08 d2.utils.events]: \u001b[0m eta: 1 day, 0:28:37  iter: 27019  total_loss: 0.355  loss_cls: 0.106  loss_box_reg: 0.230  loss_rpn_cls: 0.003  loss_rpn_loc: 0.011  time: 1.7526  data_time: 0.2828  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:21:36 d2.utils.events]: \u001b[0m eta: 1 day, 0:27:02  iter: 27039  total_loss: 0.366  loss_cls: 0.108  loss_box_reg: 0.239  loss_rpn_cls: 0.003  loss_rpn_loc: 0.011  time: 1.7455  data_time: 0.2634  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:22:03 d2.utils.events]: \u001b[0m eta: 1 day, 0:25:18  iter: 27059  total_loss: 0.374  loss_cls: 0.110  loss_box_reg: 0.257  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.7380  data_time: 0.3098  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:22:30 d2.utils.events]: \u001b[0m eta: 1 day, 0:23:51  iter: 27079  total_loss: 0.373  loss_cls: 0.111  loss_box_reg: 0.250  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.7310  data_time: 0.2956  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:22:56 d2.utils.events]: \u001b[0m eta: 1 day, 0:20:45  iter: 27099  total_loss: 0.390  loss_cls: 0.109  loss_box_reg: 0.256  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.7238  data_time: 0.2590  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:23:24 d2.utils.events]: \u001b[0m eta: 1 day, 0:19:26  iter: 27119  total_loss: 0.339  loss_cls: 0.100  loss_box_reg: 0.224  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.7174  data_time: 0.2602  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:23:51 d2.utils.events]: \u001b[0m eta: 1 day, 0:14:32  iter: 27139  total_loss: 0.372  loss_cls: 0.119  loss_box_reg: 0.244  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.7109  data_time: 0.2819  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:24:19 d2.utils.events]: \u001b[0m eta: 1 day, 0:09:44  iter: 27159  total_loss: 0.334  loss_cls: 0.099  loss_box_reg: 0.214  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.7054  data_time: 0.2868  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:24:46 d2.utils.events]: \u001b[0m eta: 1 day, 0:06:37  iter: 27179  total_loss: 0.389  loss_cls: 0.103  loss_box_reg: 0.254  loss_rpn_cls: 0.003  loss_rpn_loc: 0.011  time: 1.6997  data_time: 0.2597  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:25:14 d2.utils.events]: \u001b[0m eta: 1 day, 0:04:18  iter: 27199  total_loss: 0.407  loss_cls: 0.115  loss_box_reg: 0.268  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.6944  data_time: 0.3250  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:25:42 d2.utils.events]: \u001b[0m eta: 1 day, 0:03:20  iter: 27219  total_loss: 0.350  loss_cls: 0.097  loss_box_reg: 0.250  loss_rpn_cls: 0.002  loss_rpn_loc: 0.009  time: 1.6895  data_time: 0.3009  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:26:08 d2.utils.events]: \u001b[0m eta: 1 day, 0:00:08  iter: 27239  total_loss: 0.372  loss_cls: 0.105  loss_box_reg: 0.248  loss_rpn_cls: 0.002  loss_rpn_loc: 0.008  time: 1.6837  data_time: 0.2811  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:26:36 d2.utils.events]: \u001b[0m eta: 23:57:23  iter: 27259  total_loss: 0.369  loss_cls: 0.106  loss_box_reg: 0.242  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.6788  data_time: 0.3125  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:27:03 d2.utils.events]: \u001b[0m eta: 23:56:12  iter: 27279  total_loss: 0.363  loss_cls: 0.106  loss_box_reg: 0.243  loss_rpn_cls: 0.003  loss_rpn_loc: 0.011  time: 1.6742  data_time: 0.2689  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:27:31 d2.utils.events]: \u001b[0m eta: 23:54:58  iter: 27299  total_loss: 0.326  loss_cls: 0.091  loss_box_reg: 0.219  loss_rpn_cls: 0.002  loss_rpn_loc: 0.007  time: 1.6696  data_time: 0.2709  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:27:59 d2.utils.events]: \u001b[0m eta: 23:53:52  iter: 27319  total_loss: 0.352  loss_cls: 0.103  loss_box_reg: 0.255  loss_rpn_cls: 0.002  loss_rpn_loc: 0.008  time: 1.6652  data_time: 0.2780  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:28:26 d2.utils.events]: \u001b[0m eta: 23:52:32  iter: 27339  total_loss: 0.344  loss_cls: 0.095  loss_box_reg: 0.232  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.6607  data_time: 0.2969  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:28:52 d2.utils.events]: \u001b[0m eta: 23:50:27  iter: 27359  total_loss: 0.356  loss_cls: 0.104  loss_box_reg: 0.244  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.6555  data_time: 0.2838  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:29:19 d2.utils.events]: \u001b[0m eta: 23:49:40  iter: 27379  total_loss: 0.407  loss_cls: 0.129  loss_box_reg: 0.264  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.6514  data_time: 0.2921  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:29:47 d2.utils.events]: \u001b[0m eta: 23:50:13  iter: 27399  total_loss: 0.366  loss_cls: 0.125  loss_box_reg: 0.243  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.6472  data_time: 0.2839  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:30:14 d2.utils.events]: \u001b[0m eta: 23:50:14  iter: 27419  total_loss: 0.401  loss_cls: 0.132  loss_box_reg: 0.250  loss_rpn_cls: 0.004  loss_rpn_loc: 0.011  time: 1.6435  data_time: 0.2815  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:30:42 d2.utils.events]: \u001b[0m eta: 23:50:07  iter: 27439  total_loss: 0.417  loss_cls: 0.122  loss_box_reg: 0.265  loss_rpn_cls: 0.002  loss_rpn_loc: 0.010  time: 1.6400  data_time: 0.3294  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:31:10 d2.utils.events]: \u001b[0m eta: 23:49:28  iter: 27459  total_loss: 0.411  loss_cls: 0.129  loss_box_reg: 0.273  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.6364  data_time: 0.2859  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:31:37 d2.utils.events]: \u001b[0m eta: 23:49:20  iter: 27479  total_loss: 0.419  loss_cls: 0.127  loss_box_reg: 0.273  loss_rpn_cls: 0.004  loss_rpn_loc: 0.011  time: 1.6329  data_time: 0.3194  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:32:05 d2.utils.events]: \u001b[0m eta: 23:48:25  iter: 27499  total_loss: 0.389  loss_cls: 0.128  loss_box_reg: 0.247  loss_rpn_cls: 0.004  loss_rpn_loc: 0.010  time: 1.6297  data_time: 0.3341  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:32:32 d2.utils.events]: \u001b[0m eta: 23:47:33  iter: 27519  total_loss: 0.416  loss_cls: 0.128  loss_box_reg: 0.276  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.6260  data_time: 0.2760  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:33:00 d2.utils.events]: \u001b[0m eta: 23:47:58  iter: 27539  total_loss: 0.385  loss_cls: 0.107  loss_box_reg: 0.258  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.6229  data_time: 0.3124  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:33:27 d2.utils.events]: \u001b[0m eta: 23:47:51  iter: 27559  total_loss: 0.437  loss_cls: 0.125  loss_box_reg: 0.290  loss_rpn_cls: 0.004  loss_rpn_loc: 0.012  time: 1.6195  data_time: 0.2852  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:33:54 d2.utils.events]: \u001b[0m eta: 23:47:22  iter: 27579  total_loss: 0.393  loss_cls: 0.117  loss_box_reg: 0.253  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.6162  data_time: 0.3091  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:34:22 d2.utils.events]: \u001b[0m eta: 23:46:55  iter: 27599  total_loss: 0.401  loss_cls: 0.121  loss_box_reg: 0.266  loss_rpn_cls: 0.004  loss_rpn_loc: 0.011  time: 1.6133  data_time: 0.3040  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:34:49 d2.utils.events]: \u001b[0m eta: 23:46:22  iter: 27619  total_loss: 0.370  loss_cls: 0.111  loss_box_reg: 0.251  loss_rpn_cls: 0.002  loss_rpn_loc: 0.010  time: 1.6101  data_time: 0.2884  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:35:16 d2.utils.events]: \u001b[0m eta: 23:45:55  iter: 27639  total_loss: 0.424  loss_cls: 0.122  loss_box_reg: 0.282  loss_rpn_cls: 0.003  loss_rpn_loc: 0.012  time: 1.6071  data_time: 0.3223  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:35:44 d2.utils.events]: \u001b[0m eta: 23:45:42  iter: 27659  total_loss: 0.361  loss_cls: 0.115  loss_box_reg: 0.246  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.6045  data_time: 0.2898  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:36:12 d2.utils.events]: \u001b[0m eta: 23:45:39  iter: 27679  total_loss: 0.333  loss_cls: 0.099  loss_box_reg: 0.213  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.6017  data_time: 0.2718  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:36:38 d2.utils.events]: \u001b[0m eta: 23:44:39  iter: 27699  total_loss: 0.382  loss_cls: 0.118  loss_box_reg: 0.251  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5985  data_time: 0.3406  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:37:05 d2.utils.events]: \u001b[0m eta: 23:43:57  iter: 27719  total_loss: 0.395  loss_cls: 0.119  loss_box_reg: 0.262  loss_rpn_cls: 0.003  loss_rpn_loc: 0.007  time: 1.5957  data_time: 0.2990  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:37:33 d2.utils.events]: \u001b[0m eta: 23:43:44  iter: 27739  total_loss: 0.368  loss_cls: 0.103  loss_box_reg: 0.255  loss_rpn_cls: 0.003  loss_rpn_loc: 0.007  time: 1.5934  data_time: 0.3220  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:38:00 d2.utils.events]: \u001b[0m eta: 23:42:58  iter: 27759  total_loss: 0.369  loss_cls: 0.114  loss_box_reg: 0.243  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5904  data_time: 0.2783  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:38:27 d2.utils.events]: \u001b[0m eta: 23:41:32  iter: 27779  total_loss: 0.388  loss_cls: 0.112  loss_box_reg: 0.257  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.5878  data_time: 0.2854  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:38:54 d2.utils.events]: \u001b[0m eta: 23:40:21  iter: 27799  total_loss: 0.372  loss_cls: 0.105  loss_box_reg: 0.244  loss_rpn_cls: 0.002  loss_rpn_loc: 0.009  time: 1.5850  data_time: 0.3030  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:39:21 d2.utils.events]: \u001b[0m eta: 23:39:50  iter: 27819  total_loss: 0.378  loss_cls: 0.111  loss_box_reg: 0.254  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.5823  data_time: 0.2848  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:39:48 d2.utils.events]: \u001b[0m eta: 23:38:36  iter: 27839  total_loss: 0.417  loss_cls: 0.112  loss_box_reg: 0.260  loss_rpn_cls: 0.003  loss_rpn_loc: 0.011  time: 1.5799  data_time: 0.3450  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:40:15 d2.utils.events]: \u001b[0m eta: 23:36:52  iter: 27859  total_loss: 0.367  loss_cls: 0.109  loss_box_reg: 0.247  loss_rpn_cls: 0.004  loss_rpn_loc: 0.010  time: 1.5775  data_time: 0.2863  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:40:42 d2.utils.events]: \u001b[0m eta: 23:36:21  iter: 27879  total_loss: 0.415  loss_cls: 0.118  loss_box_reg: 0.275  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5752  data_time: 0.2840  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:41:09 d2.utils.events]: \u001b[0m eta: 23:35:19  iter: 27899  total_loss: 0.367  loss_cls: 0.104  loss_box_reg: 0.257  loss_rpn_cls: 0.002  loss_rpn_loc: 0.008  time: 1.5727  data_time: 0.2911  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:41:36 d2.utils.events]: \u001b[0m eta: 23:34:40  iter: 27919  total_loss: 0.368  loss_cls: 0.095  loss_box_reg: 0.259  loss_rpn_cls: 0.003  loss_rpn_loc: 0.011  time: 1.5706  data_time: 0.2688  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:42:04 d2.utils.events]: \u001b[0m eta: 23:34:29  iter: 27939  total_loss: 0.389  loss_cls: 0.108  loss_box_reg: 0.254  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.5684  data_time: 0.2976  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:42:31 d2.utils.events]: \u001b[0m eta: 23:33:46  iter: 27959  total_loss: 0.408  loss_cls: 0.125  loss_box_reg: 0.255  loss_rpn_cls: 0.003  loss_rpn_loc: 0.011  time: 1.5661  data_time: 0.3128  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:42:58 d2.utils.events]: \u001b[0m eta: 23:33:47  iter: 27979  total_loss: 0.402  loss_cls: 0.127  loss_box_reg: 0.272  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.5644  data_time: 0.2943  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:43:27 d2.utils.events]: \u001b[0m eta: 23:33:41  iter: 27999  total_loss: 0.416  loss_cls: 0.125  loss_box_reg: 0.269  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5625  data_time: 0.2957  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:43:55 d2.utils.events]: \u001b[0m eta: 23:33:42  iter: 28019  total_loss: 0.365  loss_cls: 0.110  loss_box_reg: 0.240  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5607  data_time: 0.3166  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:44:22 d2.utils.events]: \u001b[0m eta: 23:32:42  iter: 28039  total_loss: 0.420  loss_cls: 0.112  loss_box_reg: 0.288  loss_rpn_cls: 0.005  loss_rpn_loc: 0.010  time: 1.5585  data_time: 0.2603  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:44:50 d2.utils.events]: \u001b[0m eta: 23:32:40  iter: 28059  total_loss: 0.349  loss_cls: 0.110  loss_box_reg: 0.222  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5569  data_time: 0.2838  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:45:17 d2.utils.events]: \u001b[0m eta: 23:31:48  iter: 28079  total_loss: 0.352  loss_cls: 0.096  loss_box_reg: 0.237  loss_rpn_cls: 0.003  loss_rpn_loc: 0.007  time: 1.5550  data_time: 0.3031  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:45:45 d2.utils.events]: \u001b[0m eta: 23:32:27  iter: 28099  total_loss: 0.360  loss_cls: 0.112  loss_box_reg: 0.233  loss_rpn_cls: 0.003  loss_rpn_loc: 0.013  time: 1.5534  data_time: 0.3207  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:46:12 d2.utils.events]: \u001b[0m eta: 23:32:08  iter: 28119  total_loss: 0.415  loss_cls: 0.123  loss_box_reg: 0.270  loss_rpn_cls: 0.004  loss_rpn_loc: 0.010  time: 1.5517  data_time: 0.2627  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:46:40 d2.utils.events]: \u001b[0m eta: 23:32:18  iter: 28139  total_loss: 0.385  loss_cls: 0.106  loss_box_reg: 0.275  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5500  data_time: 0.2831  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:47:06 d2.utils.events]: \u001b[0m eta: 23:31:09  iter: 28159  total_loss: 0.349  loss_cls: 0.100  loss_box_reg: 0.226  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.5480  data_time: 0.2905  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:47:32 d2.utils.events]: \u001b[0m eta: 23:29:46  iter: 28179  total_loss: 0.403  loss_cls: 0.119  loss_box_reg: 0.277  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.5458  data_time: 0.2744  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:48:00 d2.utils.events]: \u001b[0m eta: 23:29:03  iter: 28199  total_loss: 0.392  loss_cls: 0.103  loss_box_reg: 0.261  loss_rpn_cls: 0.003  loss_rpn_loc: 0.013  time: 1.5441  data_time: 0.2991  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:48:28 d2.utils.events]: \u001b[0m eta: 23:28:36  iter: 28219  total_loss: 0.367  loss_cls: 0.120  loss_box_reg: 0.242  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5428  data_time: 0.3105  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:48:55 d2.utils.events]: \u001b[0m eta: 23:28:51  iter: 28239  total_loss: 0.350  loss_cls: 0.120  loss_box_reg: 0.220  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.5411  data_time: 0.2757  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:49:22 d2.utils.events]: \u001b[0m eta: 23:28:38  iter: 28259  total_loss: 0.389  loss_cls: 0.113  loss_box_reg: 0.251  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5394  data_time: 0.2878  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:49:50 d2.utils.events]: \u001b[0m eta: 23:28:29  iter: 28279  total_loss: 0.404  loss_cls: 0.113  loss_box_reg: 0.262  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.5382  data_time: 0.3134  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:50:17 d2.utils.events]: \u001b[0m eta: 23:28:10  iter: 28299  total_loss: 0.367  loss_cls: 0.108  loss_box_reg: 0.252  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.5369  data_time: 0.3167  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:50:44 d2.utils.events]: \u001b[0m eta: 23:26:49  iter: 28319  total_loss: 0.391  loss_cls: 0.128  loss_box_reg: 0.261  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5351  data_time: 0.3123  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:51:11 d2.utils.events]: \u001b[0m eta: 23:25:52  iter: 28339  total_loss: 0.356  loss_cls: 0.098  loss_box_reg: 0.225  loss_rpn_cls: 0.002  loss_rpn_loc: 0.009  time: 1.5333  data_time: 0.2782  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:51:37 d2.utils.events]: \u001b[0m eta: 23:25:24  iter: 28359  total_loss: 0.391  loss_cls: 0.119  loss_box_reg: 0.256  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5316  data_time: 0.3001  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:52:06 d2.utils.events]: \u001b[0m eta: 23:25:27  iter: 28379  total_loss: 0.409  loss_cls: 0.097  loss_box_reg: 0.272  loss_rpn_cls: 0.004  loss_rpn_loc: 0.015  time: 1.5307  data_time: 0.3320  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:52:33 d2.utils.events]: \u001b[0m eta: 23:25:03  iter: 28399  total_loss: 0.380  loss_cls: 0.105  loss_box_reg: 0.248  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5294  data_time: 0.3392  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:53:01 d2.utils.events]: \u001b[0m eta: 23:24:36  iter: 28419  total_loss: 0.387  loss_cls: 0.110  loss_box_reg: 0.269  loss_rpn_cls: 0.004  loss_rpn_loc: 0.009  time: 1.5281  data_time: 0.2902  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:53:29 d2.utils.events]: \u001b[0m eta: 23:24:14  iter: 28439  total_loss: 0.395  loss_cls: 0.118  loss_box_reg: 0.269  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.5272  data_time: 0.3089  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:53:56 d2.utils.events]: \u001b[0m eta: 23:23:39  iter: 28459  total_loss: 0.362  loss_cls: 0.118  loss_box_reg: 0.232  loss_rpn_cls: 0.004  loss_rpn_loc: 0.009  time: 1.5258  data_time: 0.3270  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:54:23 d2.utils.events]: \u001b[0m eta: 23:22:56  iter: 28479  total_loss: 0.372  loss_cls: 0.107  loss_box_reg: 0.252  loss_rpn_cls: 0.002  loss_rpn_loc: 0.010  time: 1.5244  data_time: 0.2778  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:54:51 d2.utils.events]: \u001b[0m eta: 23:22:45  iter: 28499  total_loss: 0.386  loss_cls: 0.115  loss_box_reg: 0.250  loss_rpn_cls: 0.002  loss_rpn_loc: 0.010  time: 1.5233  data_time: 0.2887  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:55:18 d2.utils.events]: \u001b[0m eta: 23:21:49  iter: 28519  total_loss: 0.374  loss_cls: 0.117  loss_box_reg: 0.238  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5218  data_time: 0.2869  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:55:45 d2.utils.events]: \u001b[0m eta: 23:21:10  iter: 28539  total_loss: 0.350  loss_cls: 0.106  loss_box_reg: 0.235  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5204  data_time: 0.2726  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:56:12 d2.utils.events]: \u001b[0m eta: 23:19:57  iter: 28559  total_loss: 0.365  loss_cls: 0.106  loss_box_reg: 0.260  loss_rpn_cls: 0.002  loss_rpn_loc: 0.009  time: 1.5190  data_time: 0.2976  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:56:39 d2.utils.events]: \u001b[0m eta: 23:19:47  iter: 28579  total_loss: 0.368  loss_cls: 0.115  loss_box_reg: 0.244  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5181  data_time: 0.2796  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:57:07 d2.utils.events]: \u001b[0m eta: 23:19:02  iter: 28599  total_loss: 0.372  loss_cls: 0.104  loss_box_reg: 0.247  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.5170  data_time: 0.3149  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:57:34 d2.utils.events]: \u001b[0m eta: 23:18:17  iter: 28619  total_loss: 0.370  loss_cls: 0.113  loss_box_reg: 0.244  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5157  data_time: 0.2720  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:58:01 d2.utils.events]: \u001b[0m eta: 23:17:49  iter: 28639  total_loss: 0.357  loss_cls: 0.100  loss_box_reg: 0.237  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.5146  data_time: 0.3036  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:58:29 d2.utils.events]: \u001b[0m eta: 23:16:34  iter: 28659  total_loss: 0.354  loss_cls: 0.100  loss_box_reg: 0.242  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.5134  data_time: 0.3025  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:58:57 d2.utils.events]: \u001b[0m eta: 23:16:55  iter: 28679  total_loss: 0.346  loss_cls: 0.094  loss_box_reg: 0.236  loss_rpn_cls: 0.002  loss_rpn_loc: 0.010  time: 1.5126  data_time: 0.2747  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:59:24 d2.utils.events]: \u001b[0m eta: 23:17:31  iter: 28699  total_loss: 0.399  loss_cls: 0.123  loss_box_reg: 0.253  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5116  data_time: 0.2904  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 11:59:52 d2.utils.events]: \u001b[0m eta: 23:17:15  iter: 28719  total_loss: 0.349  loss_cls: 0.108  loss_box_reg: 0.228  loss_rpn_cls: 0.003  loss_rpn_loc: 0.007  time: 1.5107  data_time: 0.3072  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:00:20 d2.utils.events]: \u001b[0m eta: 23:16:50  iter: 28739  total_loss: 0.396  loss_cls: 0.115  loss_box_reg: 0.262  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5097  data_time: 0.2556  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:00:47 d2.utils.events]: \u001b[0m eta: 23:17:04  iter: 28759  total_loss: 0.430  loss_cls: 0.143  loss_box_reg: 0.273  loss_rpn_cls: 0.004  loss_rpn_loc: 0.010  time: 1.5087  data_time: 0.2815  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:01:15 d2.utils.events]: \u001b[0m eta: 23:17:20  iter: 28779  total_loss: 0.365  loss_cls: 0.113  loss_box_reg: 0.233  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.5078  data_time: 0.2739  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:01:41 d2.utils.events]: \u001b[0m eta: 23:16:59  iter: 28799  total_loss: 0.395  loss_cls: 0.113  loss_box_reg: 0.261  loss_rpn_cls: 0.003  loss_rpn_loc: 0.011  time: 1.5066  data_time: 0.2909  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:02:09 d2.utils.events]: \u001b[0m eta: 23:15:34  iter: 28819  total_loss: 0.390  loss_cls: 0.111  loss_box_reg: 0.266  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.5056  data_time: 0.3190  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:02:36 d2.utils.events]: \u001b[0m eta: 23:15:00  iter: 28839  total_loss: 0.406  loss_cls: 0.132  loss_box_reg: 0.265  loss_rpn_cls: 0.004  loss_rpn_loc: 0.010  time: 1.5045  data_time: 0.2959  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:03:02 d2.utils.events]: \u001b[0m eta: 23:14:32  iter: 28859  total_loss: 0.372  loss_cls: 0.123  loss_box_reg: 0.228  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.5032  data_time: 0.2909  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:03:30 d2.utils.events]: \u001b[0m eta: 23:14:20  iter: 28879  total_loss: 0.363  loss_cls: 0.114  loss_box_reg: 0.227  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.5023  data_time: 0.2807  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:03:57 d2.utils.events]: \u001b[0m eta: 23:14:05  iter: 28899  total_loss: 0.379  loss_cls: 0.108  loss_box_reg: 0.262  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.5013  data_time: 0.2601  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:04:24 d2.utils.events]: \u001b[0m eta: 23:13:38  iter: 28919  total_loss: 0.404  loss_cls: 0.129  loss_box_reg: 0.264  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.5004  data_time: 0.3125  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:04:52 d2.utils.events]: \u001b[0m eta: 23:13:57  iter: 28939  total_loss: 0.392  loss_cls: 0.105  loss_box_reg: 0.266  loss_rpn_cls: 0.002  loss_rpn_loc: 0.009  time: 1.4996  data_time: 0.3087  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:05:19 d2.utils.events]: \u001b[0m eta: 23:13:25  iter: 28959  total_loss: 0.401  loss_cls: 0.131  loss_box_reg: 0.270  loss_rpn_cls: 0.005  loss_rpn_loc: 0.012  time: 1.4987  data_time: 0.2706  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:05:46 d2.utils.events]: \u001b[0m eta: 23:12:16  iter: 28979  total_loss: 0.377  loss_cls: 0.103  loss_box_reg: 0.254  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.4976  data_time: 0.2682  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:06:14 d2.utils.events]: \u001b[0m eta: 23:12:21  iter: 28999  total_loss: 0.368  loss_cls: 0.109  loss_box_reg: 0.234  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.4971  data_time: 0.3766  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:06:42 d2.utils.events]: \u001b[0m eta: 23:11:41  iter: 29019  total_loss: 0.357  loss_cls: 0.101  loss_box_reg: 0.241  loss_rpn_cls: 0.002  loss_rpn_loc: 0.009  time: 1.4962  data_time: 0.3181  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:07:09 d2.utils.events]: \u001b[0m eta: 23:11:36  iter: 29039  total_loss: 0.383  loss_cls: 0.114  loss_box_reg: 0.258  loss_rpn_cls: 0.004  loss_rpn_loc: 0.010  time: 1.4955  data_time: 0.3216  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:07:36 d2.utils.events]: \u001b[0m eta: 23:10:40  iter: 29059  total_loss: 0.372  loss_cls: 0.111  loss_box_reg: 0.229  loss_rpn_cls: 0.002  loss_rpn_loc: 0.012  time: 1.4945  data_time: 0.2912  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:08:04 d2.utils.events]: \u001b[0m eta: 23:10:37  iter: 29079  total_loss: 0.353  loss_cls: 0.110  loss_box_reg: 0.235  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.4937  data_time: 0.2648  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:08:31 d2.utils.events]: \u001b[0m eta: 23:10:04  iter: 29099  total_loss: 0.339  loss_cls: 0.087  loss_box_reg: 0.223  loss_rpn_cls: 0.002  loss_rpn_loc: 0.012  time: 1.4929  data_time: 0.2975  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:08:58 d2.utils.events]: \u001b[0m eta: 23:09:24  iter: 29119  total_loss: 0.370  loss_cls: 0.117  loss_box_reg: 0.239  loss_rpn_cls: 0.004  loss_rpn_loc: 0.010  time: 1.4919  data_time: 0.2964  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:09:25 d2.utils.events]: \u001b[0m eta: 23:08:51  iter: 29139  total_loss: 0.314  loss_cls: 0.095  loss_box_reg: 0.213  loss_rpn_cls: 0.002  loss_rpn_loc: 0.008  time: 1.4910  data_time: 0.3298  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:09:52 d2.utils.events]: \u001b[0m eta: 23:08:09  iter: 29159  total_loss: 0.355  loss_cls: 0.117  loss_box_reg: 0.238  loss_rpn_cls: 0.002  loss_rpn_loc: 0.007  time: 1.4900  data_time: 0.2862  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:10:19 d2.utils.events]: \u001b[0m eta: 23:08:27  iter: 29179  total_loss: 0.389  loss_cls: 0.110  loss_box_reg: 0.247  loss_rpn_cls: 0.002  loss_rpn_loc: 0.008  time: 1.4893  data_time: 0.2661  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:10:46 d2.utils.events]: \u001b[0m eta: 23:07:56  iter: 29199  total_loss: 0.370  loss_cls: 0.115  loss_box_reg: 0.242  loss_rpn_cls: 0.003  loss_rpn_loc: 0.007  time: 1.4883  data_time: 0.2995  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:11:13 d2.utils.events]: \u001b[0m eta: 23:07:14  iter: 29219  total_loss: 0.363  loss_cls: 0.103  loss_box_reg: 0.230  loss_rpn_cls: 0.003  loss_rpn_loc: 0.011  time: 1.4876  data_time: 0.2795  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:11:41 d2.utils.events]: \u001b[0m eta: 23:06:58  iter: 29239  total_loss: 0.368  loss_cls: 0.113  loss_box_reg: 0.238  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.4869  data_time: 0.2860  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:12:08 d2.utils.events]: \u001b[0m eta: 23:06:47  iter: 29259  total_loss: 0.349  loss_cls: 0.107  loss_box_reg: 0.234  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.4862  data_time: 0.2961  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:12:35 d2.utils.events]: \u001b[0m eta: 23:05:45  iter: 29279  total_loss: 0.404  loss_cls: 0.118  loss_box_reg: 0.282  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.4854  data_time: 0.2850  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:13:03 d2.utils.events]: \u001b[0m eta: 23:05:12  iter: 29299  total_loss: 0.400  loss_cls: 0.104  loss_box_reg: 0.257  loss_rpn_cls: 0.004  loss_rpn_loc: 0.010  time: 1.4848  data_time: 0.3409  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:13:30 d2.utils.events]: \u001b[0m eta: 23:05:08  iter: 29319  total_loss: 0.348  loss_cls: 0.106  loss_box_reg: 0.231  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.4841  data_time: 0.2913  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:13:57 d2.utils.events]: \u001b[0m eta: 23:05:18  iter: 29339  total_loss: 0.414  loss_cls: 0.123  loss_box_reg: 0.266  loss_rpn_cls: 0.003  loss_rpn_loc: 0.011  time: 1.4833  data_time: 0.2803  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:14:24 d2.utils.events]: \u001b[0m eta: 23:04:59  iter: 29359  total_loss: 0.390  loss_cls: 0.130  loss_box_reg: 0.243  loss_rpn_cls: 0.004  loss_rpn_loc: 0.008  time: 1.4824  data_time: 0.2788  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:14:51 d2.utils.events]: \u001b[0m eta: 23:03:53  iter: 29379  total_loss: 0.359  loss_cls: 0.111  loss_box_reg: 0.245  loss_rpn_cls: 0.003  loss_rpn_loc: 0.007  time: 1.4816  data_time: 0.2666  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:15:18 d2.utils.events]: \u001b[0m eta: 23:03:18  iter: 29399  total_loss: 0.381  loss_cls: 0.115  loss_box_reg: 0.270  loss_rpn_cls: 0.002  loss_rpn_loc: 0.010  time: 1.4808  data_time: 0.2886  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:15:45 d2.utils.events]: \u001b[0m eta: 23:01:40  iter: 29419  total_loss: 0.337  loss_cls: 0.105  loss_box_reg: 0.242  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.4800  data_time: 0.2928  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:16:12 d2.utils.events]: \u001b[0m eta: 23:00:52  iter: 29439  total_loss: 0.369  loss_cls: 0.115  loss_box_reg: 0.241  loss_rpn_cls: 0.002  loss_rpn_loc: 0.008  time: 1.4794  data_time: 0.2785  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:16:38 d2.utils.events]: \u001b[0m eta: 22:59:24  iter: 29459  total_loss: 0.361  loss_cls: 0.102  loss_box_reg: 0.245  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.4784  data_time: 0.2900  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:17:06 d2.utils.events]: \u001b[0m eta: 22:59:45  iter: 29479  total_loss: 0.376  loss_cls: 0.112  loss_box_reg: 0.248  loss_rpn_cls: 0.002  loss_rpn_loc: 0.008  time: 1.4778  data_time: 0.2919  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:17:34 d2.utils.events]: \u001b[0m eta: 22:59:48  iter: 29499  total_loss: 0.368  loss_cls: 0.105  loss_box_reg: 0.236  loss_rpn_cls: 0.002  loss_rpn_loc: 0.009  time: 1.4775  data_time: 0.3320  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:18:03 d2.utils.events]: \u001b[0m eta: 23:00:23  iter: 29519  total_loss: 0.375  loss_cls: 0.111  loss_box_reg: 0.240  loss_rpn_cls: 0.003  loss_rpn_loc: 0.008  time: 1.4771  data_time: 0.3256  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:18:30 d2.utils.events]: \u001b[0m eta: 23:00:18  iter: 29539  total_loss: 0.359  loss_cls: 0.103  loss_box_reg: 0.236  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.4765  data_time: 0.2688  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:18:58 d2.utils.events]: \u001b[0m eta: 23:00:42  iter: 29559  total_loss: 0.400  loss_cls: 0.107  loss_box_reg: 0.259  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.4759  data_time: 0.2771  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:19:25 d2.utils.events]: \u001b[0m eta: 22:59:24  iter: 29579  total_loss: 0.351  loss_cls: 0.103  loss_box_reg: 0.227  loss_rpn_cls: 0.002  loss_rpn_loc: 0.008  time: 1.4752  data_time: 0.2804  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:19:52 d2.utils.events]: \u001b[0m eta: 22:58:56  iter: 29599  total_loss: 0.360  loss_cls: 0.116  loss_box_reg: 0.236  loss_rpn_cls: 0.005  loss_rpn_loc: 0.011  time: 1.4746  data_time: 0.2872  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:20:19 d2.utils.events]: \u001b[0m eta: 22:58:34  iter: 29619  total_loss: 0.365  loss_cls: 0.106  loss_box_reg: 0.248  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 1.4740  data_time: 0.2882  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:20:47 d2.utils.events]: \u001b[0m eta: 22:58:39  iter: 29639  total_loss: 0.365  loss_cls: 0.108  loss_box_reg: 0.221  loss_rpn_cls: 0.004  loss_rpn_loc: 0.011  time: 1.4736  data_time: 0.3227  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:21:14 d2.utils.events]: \u001b[0m eta: 22:57:33  iter: 29659  total_loss: 0.389  loss_cls: 0.106  loss_box_reg: 0.267  loss_rpn_cls: 0.003  loss_rpn_loc: 0.012  time: 1.4727  data_time: 0.2769  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:21:40 d2.utils.events]: \u001b[0m eta: 22:56:10  iter: 29679  total_loss: 0.396  loss_cls: 0.110  loss_box_reg: 0.282  loss_rpn_cls: 0.002  loss_rpn_loc: 0.009  time: 1.4718  data_time: 0.2714  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:22:07 d2.utils.events]: \u001b[0m eta: 22:54:37  iter: 29699  total_loss: 0.374  loss_cls: 0.106  loss_box_reg: 0.243  loss_rpn_cls: 0.002  loss_rpn_loc: 0.008  time: 1.4712  data_time: 0.3392  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:22:35 d2.utils.events]: \u001b[0m eta: 22:54:09  iter: 29719  total_loss: 0.384  loss_cls: 0.107  loss_box_reg: 0.248  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.4708  data_time: 0.2732  lr: 0.020000  max_mem: 11512M\n",
            "\u001b[32m[04/26 12:23:02 d2.utils.events]: \u001b[0m eta: 22:53:30  iter: 29739  total_loss: 0.372  loss_cls: 0.104  loss_box_reg: 0.242  loss_rpn_cls: 0.003  loss_rpn_loc: 0.009  time: 1.4702  data_time: 0.3094  lr: 0.020000  max_mem: 11512M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzn4GxY1vD1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7odpsbAbXhkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIb8BMtVXjAC",
        "colab_type": "text"
      },
      "source": [
        "# Inference & evaluation using the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIiIVbrdqtA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PRETRAIN_MODEL_PATH = '/content/drive/My Drive/Final-project/detectron2'\n",
        "\n",
        "cfg.MODEL.WEIGHTS = os.path.join(PRETRAIN_MODEL_PATH, \"model_0023999.pth\")\n",
        "# cfg.MODEL.WEIGHTS = os.path.join('/content/output/model_final.pth')\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n",
        "cfg.DATASETS.TEST = (\"my_dataset_val2\", )\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRyHtOGBZY6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"my_dataset_val2\", {}, \"/content/drive/My Drive/Final-project/detectron2/val/output_val.json\", \"/content/drive/My Drive/Final-project/detectron2/val/validation_images\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twYTd2EeZY9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ingre_val_metadata = MetadataCatalog.get(\"my_dataset_val2\")\n",
        "from detectron2.data import DatasetCatalog\n",
        "dataset_val_dicts = DatasetCatalog.get(\"my_dataset_val2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7li0DYx09Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_val_dicts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeLis72Tqe3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "\n",
        "# for d in random.sample(dataset_val_dicts, 3):\n",
        "for d in dataset_val_dicts:\n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=ingre_val_metadata, \n",
        "                   scale=0.8\n",
        "    )\n",
        "    print(outputs)\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(v.get_image()[:, :, ::-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmouY2GXt44F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im = cv2.imread('/content/drive/My Drive/Final-project/detectron2/test_image_03.jpg')\n",
        "outputs = predictor(im)\n",
        "v = Visualizer(im[:, :, ::-1],\n",
        "                metadata=ingre_val_metadata, \n",
        "                scale=0.8\n",
        ")\n",
        "print(outputs)\n",
        "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(v.get_image()[:, :, ::-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3F7Gf_WvQip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVbzR9Lhws_K",
        "colab_type": "text"
      },
      "source": [
        "## train image check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q6pLltRvRZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# i = 0\n",
        "# for d in dataset_dicts:\n",
        "#     ## onion 이미지 확인(우선 100개만)\n",
        "#     if d['annotations'][0]['category_id'] == 2:\n",
        "#         img = cv2.imread(d[\"file_name\"])\n",
        "#         visualizer = Visualizer(img[:, :, ::-1], metadata=ingre_metadata, scale=0.5)\n",
        "#         vis = visualizer.draw_dataset_dict(d)\n",
        "#         cv2_imshow(vis.get_image()[:, :, ::-1])\n",
        "#         i+=1\n",
        "#     if i == 10:\n",
        "#         break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t3F8z30vQlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55I5vmG_wprK",
        "colab_type": "text"
      },
      "source": [
        "## evaluate its performance using AP metric implemented in COCO API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2DPkAyCrner",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"my_dataset_val2\", cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
        "val_loader = build_detection_test_loader(cfg, \"my_dataset_val2\")\n",
        "inference_on_dataset(trainer.model, val_loader, evaluator)\n",
        "# another equivalent way is to use trainer.test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-vEwmztqfBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1e7CzTCLXLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}